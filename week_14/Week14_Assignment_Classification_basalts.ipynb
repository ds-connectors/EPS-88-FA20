{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of basalt source\n",
    "\n",
    "## Import scientific python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import copy\n",
    "\n",
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/volcanic-tectonics.png\" width = 800 align = 'center'>\n",
    "\n",
    "In this assignment you will continue your investigation of igneous geochemistry data. Igneous rocks are those that crystallize from cooling magma. Different magmas have different compositions associated with their origin as we explored a few weeks ago. During class today, we will continue to focus on data from mafic lava flows (these are called basalts and are the relatively low silica, high iron end of what we looked at in week 7).\n",
    "\n",
    "> Igneous rocks form in a wide variety of tectonic settings,\n",
    "including mid-ocean ridges, ocean islands, and volcanic\n",
    "arcs. It is a problem of great interest to igneous petrologists\n",
    "to recover the original tectonic setting of mafic rocks of the\n",
    "past. When the geological setting alone cannot unambiguously\n",
    "resolve this question, the chemical composition of\n",
    "these rocks might contain the answer. The major, minor,\n",
    "and trace elemental composition of basalts shows large\n",
    "variations, for example as a function of formation depth\n",
    "(e.g., Kushiro and Kuno, 1963) --- *Vermeesch (2006)*\n",
    "\n",
    "For this analysis we are going to use a dataset that was compiled in \n",
    "\n",
    "Vermeesch (2006) Tectonic discrimination of basalts with classification trees, *Geochimica et Cosmochimica Acta*  https://doi.org/10.1016/j.gca.2005.12.016\n",
    "\n",
    "These data were grouped into 3 categories:\n",
    "\n",
    "- 256 ***Island arc basalts (IAB)*** from the Aeolian, Izu-Bonin, Kermadec, Kurile, Lesser Antilles, Mariana, Scotia, and Tonga arcs.\n",
    "- 241 ***Mid-ocean ridge (MORB)*** samples from the East Pacific Rise, Mid Atlantic Ridge, Indian Ocean, and Juan de Fuca Ridge.\n",
    "- 259 ***Ocean-island (OIB)*** samples from St. Helena, the Canary, Cape Verde, Caroline, Crozet, Hawaii-Emperor, Juan Fernandez, Marquesas, Mascarene, Samoan, and Society islands.\n",
    "\n",
    "**Let's look at the illustration above and determine where each of these settings are within a plate tectonic context**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "\n",
    "The data are from the supplemental materials of the Vermeesch (2006) paper. The samples are grouped by affinity MORB, OIB, and IAB. They are additionally assigned affinity codes and colors from the default matplotlib cycle:\n",
    "\n",
    "|affinity| affinity code | color |\n",
    "|--------|---------------|-------|\n",
    "| MORB| 0 | C0\n",
    "| OIB |  1 | C1\n",
    "| IAB |  2 | C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_data = pd.read_csv('./data/Vermeesch2006.csv')\n",
    "basalt_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MORB_data = basalt_data[basalt_data['affinity']=='MORB']\n",
    "OIB_data = basalt_data[basalt_data['affinity']=='OIB']\n",
    "IAB_data = basalt_data[basalt_data['affinity']=='IAB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can geochemical data be used to classify the tectonic setting?\n",
    "\n",
    "These data are labeled. The author already determined what setting these basalts came from. However, is there a way that we could use these labeled data to determine the setting for an unknown basalt?\n",
    "\n",
    "A paper published in 1982 proposed that the elements titanium and vanadium were particular good at giving insight into tectonic setting. The details of why are quite complicated and can be summarized as \"the depletion of V relative to Ti is a function of the fO2 of the magma and its source, the degree of partial melting, and subsequent fractional crystallization.\" If you take EPS100B you will learn more about the fundamentals behind this igneous petrology. *For the moment you can consider the working hypothesis behind this classification to that different magmatic environments have differences in oxidation states that are reflected in Ti vs V ratios.*\n",
    "\n",
    "Shervais, J.W. (1982) Ti-V plots and the petrogenesis of modern and ophiolitic lavas *Earth and Planetary Science Letters* https://doi.org/10.1016/0012-821X(82)90120-0\n",
    "\n",
    "### Plot TiO2 (wt%) vs V (ppm)\n",
    "\n",
    "**Make a scatter plot of TiO2 (wt%) vs V (ppm) with the markers color-coded by affinity. Include axis labels and a legend.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification by-eye method\n",
    "\n",
    "In order to classify the basalt into their affinity based on titanium and vanadium concentrations, we can use a classification method.\n",
    "\n",
    "The goal here is to be able to make an inference of what environment an unknown basalt formed in based on comparison to these data.\n",
    "\n",
    "Let's say that we have three points where there affinity is unknown.\n",
    "- point 1 has TiO2 of 4% and V concentration of 300 ppm\n",
    "- point 2 has TiO2 of 1% and V concentration of 350 ppm\n",
    "- point 3 has TiO2 of 1.9% and V concentration of 200 ppm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_1_TiO2 = 4\n",
    "point_1_V = 300\n",
    "point_2_TiO2 = 1\n",
    "point_2_V = 350\n",
    "point_3_TiO2 = 1.9\n",
    "point_3_V = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(MORB_data['TiO2 (wt%)'],MORB_data['V (ppm)'],label='mid-ocean ridge',edgecolors='black')\n",
    "plt.scatter(OIB_data['TiO2 (wt%)'],OIB_data['V (ppm)'],label='ocean island',edgecolors='black')\n",
    "plt.scatter(IAB_data['TiO2 (wt%)'],IAB_data['V (ppm)'],label='island arc',edgecolors='black')\n",
    "plt.scatter(point_1_TiO2,point_1_V,label='unknown point 1',color='cyan',edgecolors='black',marker='d',s=100)\n",
    "plt.scatter(point_2_TiO2,point_2_V,label='unknown point 2',color='magenta',edgecolors='black',marker='>',s=100)\n",
    "plt.scatter(point_3_TiO2,point_3_V,label='unknown point 2',color='yellow',edgecolors='black',marker='s',s=100)\n",
    "plt.xlabel('TiO2 (wt%)')\n",
    "plt.ylabel('V (ppm)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***WRITE HOW YOU THINK THEY SHOULD BE CLASSIFIED HERE***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbors Classification\n",
    "\n",
    "In nearest neighbors classification, classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point. There are different ways this can be done and can be weighted.\n",
    "\n",
    "### Filter the data to ones that have Ti and V data\n",
    "\n",
    "**Filter out the rows with NaN values in the `TiO2 (wt%)` or `'V (ppm)'` columns** (i.e. keep rows where its is not true that both of these are nans, you may need `~` and `isna()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the data\n",
    "\n",
    "Given that the nearest neighbor is a distance and TiO2 and V have ranges that are so different (in part because of different units) you need to normalize the data. Divide the 'TiO2 (wt%)' by the maximum 'TiO2 (wt%)' to get a value between 0 and 1. Do the same for V (ppm) as well.\n",
    "\n",
    "**Add to your filtered dataframe a column called Ti_norm that is normalized TiO2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make a column called V_norm that is normalized vanadium.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make a scatter plot of Ti_norm vs V_norm that is colored by affinity.** It should look a lot like the previous scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing arrays of the data\n",
    "\n",
    "**Make a 2 x n array of the TiO2 (wt%) and V (ppm) values (where n is the number of data points) and a 1 x n array of the classifications (the tectonic affinities).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your classifier\n",
    "\n",
    "**Construct a classifier that uses the 5 nearest neighbors (`n_neighbors=5`) and weight points by the inverse of their distance (`weights='distance'`) such that closer neighbors of a query point will have a greater influence than neighbors which are further away.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit/train the classifier\n",
    "\n",
    "**Feed the array of the data and the array of the classification in a `.fit` function preformed on the classifier object.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the mystery points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_1_TiO2_norm = 4/np.max(basalt_data['TiO2 (wt%)'])\n",
    "point_1_V_norm = 300/np.max(basalt_data['V (ppm)'])\n",
    "point_2_TiO2_norm = 1/np.max(basalt_data['TiO2 (wt%)'])\n",
    "point_2_V_norm = 350/np.max(basalt_data['V (ppm)'])\n",
    "point_3_TiO2_norm = 1.9/np.max(basalt_data['TiO2 (wt%)'])\n",
    "point_3_V_norm = 200/np.max(basalt_data['V (ppm)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the tectonic affinity of the mystery points using the neighbors classifier\n",
    "\n",
    "**Use `.predict` to predict the tectonic affinity of these normalized mystery points using the trained neighbors alorithm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit/train using the basalt_affinity_code rather than the string names and use it to predict basalt_affinity_code for the mystery points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the decision boundary\n",
    "\n",
    "**Make a 101 x 101 grid of x and y values between 0 and 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.linspace(0, 1, 101),\n",
    "                     np.linspace(0, 1, 101))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify the grid\n",
    "\n",
    "**Use `.predict` to predict the tectonic affinity of these grid points using the trained neighbors alorithm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the classification boundaries by plotting the grid points colorcoded by their classifation. Add a scatter plot of the observed (normalized) data points points colorcoded by their labels on top.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing\n",
    "\n",
    "How good is your nearest neighbor classifier? To answer this you'll need to find out how frequently your classifications are correct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a training and testing data set\n",
    "\n",
    "There are 514 rows with TiO2 and V data. Use a random half of them for training and the other half for testing. To do this, shuffle all the rows, take the first 257 as the training set, and the remaining 257 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a randomly ordered dataframe from the initial one\n",
    "randomized_basalt_data = basalt_data_Ti_V.sample(frac=1) \n",
    "\n",
    "# Take the first 257 data points to use for \"training\"\n",
    "training_data = copy.deepcopy(randomized_basalt_data.iloc[0:257])\n",
    "\n",
    "# Use the rest to apply our machine learning on\n",
    "remaining_data = copy.deepcopy(randomized_basalt_data.iloc[257:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_Ti_V_training = training_data[['Ti_norm', 'V_norm']].values\n",
    "basalt_Ti_V_remaining = remaining_data[['Ti_norm', 'V_norm']].values\n",
    "basalt_affinity_training = training_data['affinity code'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_neighbors.fit(basalt_Ti_V_training, basalt_affinity_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the classification regions fit with half the data\n",
    "\n",
    "Send the grid to the classifier to see the classification regions and decision boundary that has been fit with half of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_classes = classifier_neighbors.predict(grid)\n",
    "grid_classes = grid_classes.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.pcolormesh(xx, yy, grid_classes, cmap=cmap)\n",
    "plt.xlabel('Ti_norm')\n",
    "plt.ylabel('V_norm')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.gca().set_aspect('equal', 'box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the remaining data (test data) to the classification regions\n",
    "\n",
    "Place the test data on this graph and you can see at once that while the classifier got many of the points right, there are some mis-classified points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = ListedColormap(['C0', 'C1', 'C2'])\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.pcolormesh(xx, yy, grid_classes, cmap=cmap)\n",
    "\n",
    "plt.scatter(remaining_data['Ti_norm'],remaining_data['V_norm'],\n",
    "                           color=remaining_data['color'],edgecolors='black')\n",
    "\n",
    "plt.xlabel('Ti_norm')\n",
    "plt.ylabel('V_norm')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.gca().set_aspect('equal', 'box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the accuracy of the classifier\n",
    "\n",
    "Since the test set was chosen randomly from the original sample it should preform with similar accuracy on the overall population. Let's calculate the success rate of the classification.\n",
    "\n",
    "Input the remaining data (test data) to the classifier and then assign these classified affinities to a new column in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_classes = classifier_neighbors.predict(basalt_Ti_V_remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_data['predicted_class'] = remaining_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a new column of the classified affinities for the test data. You also have the actually affinities given that the data were originally labeled with classifications. How often do they agree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_data['correct_assignment'] = remaining_data['predicted_class'].eq(remaining_data['affinity code'])\n",
    "remaining_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_data['correct_assignment'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using scikit-learn functions to get an accuracy score of this nearest neighbor approach\n",
    "\n",
    "Given that this approach of randomly splitting the data into training and test groups is quite common in machine learning classification, there are built-in convenience functions that can be used to more compactly do the same operations that you did above: `train_test_split` and `accuracy_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# split the data with 50% in each set\n",
    "X1, X2, y1, y2 = train_test_split(basalt_Ti_V, basalt_affinity_code,train_size=0.5)\n",
    "\n",
    "# fit the model on one set of data\n",
    "classifier_neighbors.fit(X1, y1)\n",
    "\n",
    "# evaluate the model on the second set of data\n",
    "y2_model = classifier_neighbors.predict(X2)\n",
    "accuracy_score(y2, y2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other classification algorithms\n",
    "\n",
    "If you go to the scikit-learn homepage you will find many available classifiers: https://scikit-learn.org/stable/index.html. They are nicely illustrated in this code from the scikit-learn documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code source: Gaël Varoquaux\n",
    "#              Andreas Müller\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable\n",
    "            ]\n",
    "\n",
    "figure = plt.figure(figsize=(27, 9))\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "               edgecolors='k')\n",
    "    # Plot the testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n",
    "               edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                   edgecolors='k')\n",
    "        # Plot the testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                   edgecolors='k', alpha=0.6)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a word of warning, we shouldn't get too carried away. Clearly, there are complexities related to this approach (our accuracy scores aren't that high). Shervais notes that: \n",
    "> \"More specific evaluation of the tectonic setting of these and other ophiolites requires\n",
    "application of detailed geologic and petrologic data as well as geochemistry. The Ti/V discrimination diagram, however,\n",
    "is a potentially powerful adjunct to these techniques.\"\n",
    "\n",
    "Additionally, we would like to be able to assign physical processes to the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore other geochemical parameters of the data and build additional classifiers\n",
    "\n",
    "**Tasks for you to complete**\n",
    "\n",
    "- Use the seaborn library and use the sns.pairplot function to make cross-plots of other parameters (https://seaborn.pydata.org/generated/seaborn.pairplot.html)\n",
    "- *Are there other geochemical parameters that you can use as a classifier that are as good or better than the Ti/V classifier?* Implement another classifier using the algorithm type of your choosing and determine its accuracy using a training set and a test set to address this question. ***scikit-learn will not be happy with missing values so filter out missing values beforehand***. ***Remember that if you are using the nearest neighbor approach that you need to normalize the data.***\n",
    "- Build a classifier that uses more than 2 dimensions, as we did in class. Build a classifier that uses 3 or more parameters instead of 2. When you fit the classifier you provide an array that has:\n",
    "\n",
    "    `[[data_a_point1,data_b_point1,data_c_point1],[data_a_point2,data_b_point2,data_c_point2]]`\n",
    "\n",
    "    and then an array of type:\n",
    "\n",
    "    `[point1_type, point2_type]`\n",
    "    \n",
    "    While we had Ti and V in the first array you could have these geochemical data and more so that instead of being 2 x n, it would be 3 x n or 4 x n (where n is the number of data point values) and 3 or 4 is the number of geochemical parameters you use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn in the Notebook\n",
    "\n",
    "**Export as HTML and upload to bCourses.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
