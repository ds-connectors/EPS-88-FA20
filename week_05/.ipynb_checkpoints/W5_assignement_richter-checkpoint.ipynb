{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Earthquake Occurrence Statistics: Gutenberg-Richter Law\n",
    "\n",
    "Declustering seismic data.\n",
    "\n",
    "**In class we:**\n",
    "- Loaded a Bay Area seismic catalog around the Loma Prieta Quake.\n",
    "- Computed the distance and time interval between Loma Prieta quake and subsequant earthquakes to indentify aftershocks.\n",
    "- Filtered the aftershocks from the catalog and looked at their distribution.\n",
    "\n",
    "**In this homwork:**\n",
    "- Load a Bay Area seismic catalog since 1900.\n",
    "- Define a function that uses a `for` loop to identify aftershocks for all the main shock events.\n",
    "- Remove the aftershocks from the catalog (decluster).\n",
    "- Explore the distribution of main shock events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell as it is to setup your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency of earthquake recurrence as a function of magnitude has been a focus of seismological research since Gutenberg and Richters pioneering work (Gutenberg and Richter, 1949). The evidence shows that the numbers of earthquaks in a given time period scales logarthmically with magnitude. To first order there are 10 times more magnitude 5 earthquakes compared to magnitude 6 events, and 10 times more magnitude 4 earthquakes compared to magnitude 5s.\n",
    "\n",
    "Gutenberg and Richter found that when the logarithm of the number of earthquakes is plotted vs. magnitude that the distribution may be plotted as the line, log(N)=A+Bm, where N is the number of earthquakes, m is the magnitude and A and B are the slope and intercept of a line. For the example described above the B-value is equal to -1 (there are 10 times fewer earthquakes for an increase of one magnitude unit). An important point to keep in mind that these parameters are based on a primary earthquake catalog in which aftershocks have been removed. **The process of aftershock removal is called declustering.**\n",
    "\n",
    "Why is this important? The A- and B-values are often used to characterize the rates of earthquakes to identify regional variability. The B-value (slope parameter) is often used to distinquish between 'normal' and 'swarm-like' earthquake behavior. In geothermal areas it has been observed that the earthquake distribution is richer in small earthquakes indicating a B-value significantly less than -1. \n",
    "\n",
    "Gutenberg Richter is also used to characterize seismic hazard in a region by defining the annual rate of earthquake occurrence. In this notebook you will analyze a earthquake catalog downloaded from the Northern California Earthquake Data Center from around the Berkeley Campus. You will learn how to decluster the seismicity catalog, estimate the Gutenberg Richter A- and B- values, and estimate the annual recurrence rates of large earthquake in the region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Earthquake Catalog\n",
    "\n",
    "Load the .csv data file 'data/bay_area_anss_1900_2020.csv' of all the earthquakes 1900 - 2020 in the ANSS (Advanced National Seismic System) catalog from around Berkeley (latitudes 37.0-38.75° and longitude -123.35 to -121.5°; [http://ncedc.org/anss/catalog-search.html](http://ncedc.org/anss/catalog-search.html)). Convert the column DateTime to a datetime object with `pd.to_datetime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "bay_catalog=pd.read_csv('')\n",
    "bay_catalog['DateTime'] = \n",
    "bay_catalog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create data arrays, it will speed up our loops later\n",
    "year=bay_catalog['DateTime'].dt.year\n",
    "month=bay_catalog['DateTime'].dt.month\n",
    "day=bay_catalog['DateTime'].dt.day\n",
    "lat=bay_catalog['Latitude'].values\n",
    "lon=bay_catalog['Longitude'].values\n",
    "mag=bay_catalog['Magnitude'].values\n",
    "nevt=len(year)        #number of events "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>**_Concept question:_**</font> How many total events are in this catalog before we remove the aftershocks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map the Raw Earthquake Catalog\n",
    "\n",
    "On a map of the Bay Area plot the location of events in the raw catalog. Scale the marker color and size by magnitude. Add a colorbar with a label. \n",
    "\n",
    "<font color=goldenrod>**_Code for you to write_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a Map of the earthquake catalog\n",
    "\n",
    "# Set Corners of Map\n",
    "lat0=37.0\n",
    "lat1=38.75\n",
    "lon0=-123.25\n",
    "lon1=-121.5\n",
    "tickstep=0.25 #for axes\n",
    "latticks=np.arange(lat0,lat1+tickstep,tickstep)\n",
    "lonticks=np.arange(lon0,lon1+tickstep,tickstep)\n",
    "\n",
    "# coordinates for UC Berkeley\n",
    "Berk_lat = 37.8716\n",
    "Berk_lon = -122.2727\n",
    "\n",
    "# make plot object with ticks, coastlines, etc.\n",
    "plt.figure(1,(10,10))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_extent([lon0, lon1, lat0, lat1], crs=ccrs.PlateCarree())\n",
    "ax.coastlines(resolution='10m',linewidth=1)\n",
    "ax.set_xticks(lonticks)\n",
    "ax.set_yticks(latticks, crs=ccrs.PlateCarree())\n",
    "ax.set(xlabel='Longitude', ylabel='Latitude',title='Raw Catalog')\n",
    "\n",
    "\n",
    "# Sort by magnitude to plot largest events on top\n",
    "bay_catalog_sorted = ...\n",
    "#exponent to scale marker size\n",
    "z=...\n",
    "\n",
    "plt.scatter(..., ..., s=..., c=..., cmap='plasma',alpha=0.4,marker='o') # plot circles on EQs\n",
    "plt.plot(Berk_lon,Berk_lat,'s',color='limegreen',markersize=8)  # plot green square on Berkeley Campus\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Magnitude vs. Time for the Raw Catalog\n",
    "\n",
    "Plot magnitude vs. time for the raw catalog as we did in-class. Use the `alpha = 0.2` argument in `plot` to make the markers transparent. Add labels and a title.\n",
    "\n",
    "<font color=goldenrod>**_Code for you to write_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot magnitude vs. time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>**_Concept questions:_**</font> The first event in the catalog is a few years after the Great 1906 San Francisco Earthquake. Read a bit about it here [The Great 1906 San Francisco Earthquake](https://earthquake.usgs.gov/earthquakes/events/1906calif/18april/). That event inspired scientists to start recording the earthquake catalog. But early instruments for measuring earthquakes were not as good as modern seismometers. When are there events missing from this catalog i.e. are there large gaps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Histogram of Magnitude for the Raw Catalog\n",
    "\n",
    "Plot a histogram of the magnitudes in the raw catalog, with a log y-axis. Be sure to label you axes.\n",
    "\n",
    "<font color=goldenrod>**_Code for you to write_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the B-value of the Gutenberg-Richter Law for the raw catalog\n",
    "\n",
    "Run this code as is and inspect the B-value (i.e. a slope fit to the above histogram) for the Bay Area catalog before you remove the aftershocks (i.e. non-independent events)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the observed log10 number of events per year as function of magnitude (data)\n",
    "min_mag=0.0\n",
    "max_mag=6.9\n",
    "m_raw=np.arange(min_mag,max_mag,0.1)\n",
    "N_raw=np.zeros(len(m_raw))\n",
    "numyr=2020-1910\n",
    "for i in range(0,len(m_raw),1):\n",
    "    N_raw[i]=np.log10(np.count_nonzero(mag >= m_raw[i])/numyr)\n",
    "    \n",
    "# Solve for Model Parameters\n",
    "A_raw = np.column_stack((np.ones(len(m_raw)), m_raw))\n",
    "trans_A_raw = np.transpose(A_raw)\n",
    "ATA_raw = trans_A_raw @ A_raw\n",
    "ATN_raw = trans_A_raw @ np.transpose([N_raw])\n",
    "inv_raw = np.linalg.inv(ATA_raw)\n",
    "soln_raw = inv_raw @ ATN_raw\n",
    "\n",
    "print('The raw data B-value: %7.3f'%(soln_raw[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declustering - window method\n",
    "\n",
    "For each earthquake in the catalog with magnitude M, the subsequent earthquakes are determined to be aftershocks if they occur within a distance L(M) and time interval T(M). An example of aftershock windows from Gardner and Knopoff (1974) is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/aftershock_windows.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An approximation of the windows sizes according to Gardner and Knopoff (1974) is shown in equation the equation below. Using this approximation makes programming a windowing algorithm easier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/window_approx.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Determine the number of days from the first quake in the catalog\n",
    "\n",
    "To build your algorithm to identify aftershock using these windows you need to convert the year-month-day formate of dates to a timeline in number of days. You'll do this using the function `datetime.date` which for a given year, month, and day returns a datetime class object, which can be used to compute the time interval in days from the **first event in the catalog**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the number of days from the first event\n",
    "days=np.zeros(nevt) # initialize the size of the array days\n",
    "\n",
    "for i in range(0,nevt,1):\n",
    "    d0 = datetime.date(year[0], month[0], day[0])\n",
    "    d1 = datetime.date(year[i], month[i], day[i])\n",
    "    delta = d1 - d0\n",
    "    days[i]=delta.days # fill days in with the number of days since the first event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to compute the distance between to geographic points on a sphere\n",
    "\n",
    "Your algorithm to identify aftershock also needs the distance between earthquakes. So as we did in-class define a function for computing the distance between two geographic points that you can then call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function computes the spherical earth distance between to geographic points and is used in the\n",
    "#declustering algorithm below\n",
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "\n",
    "    All args must be of equal length.\n",
    "    \n",
    "    The first pair can be singular and the second an array\n",
    "\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2]) # convert degrees lat, lon to radians\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2  # great circle inside sqrt\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))   # great circle angular separation\n",
    "    km = 6371.0 * c   # great circle distance in km, earth radius = 6371.0 km\n",
    "    return km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declustering Algorithm Time!\n",
    "\n",
    "You'll build a `for` loop for indentifying aftershocks in the seismic catalog. In-class we just found the aftershocks for one main shock event. Here you will add a `for` loop that iterates through the whole catalog and finds all events that can be cassified as aftershocks. Replace the xxx's in the code below:\n",
    "\n",
    "<font color=goldenrod>**_Code for you to write_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decluster the Catalog  Note: This cell may take a few minute to complete\n",
    "cnt=0 # initialize a counting variable\n",
    "save=np.zeros((1,10000000),dtype=int) # initialize a counting variable\n",
    "for xxx   # step through EQ catalog\n",
    "    # logical if statements to incorporate definitions of Dtest and Ttest aftershock window bounds\n",
    "    Dtest=xxx   # distance bounds\n",
    "    if mag[i] >= 6.5:\n",
    "        Ttest=xxx  # aftershock time bounds for M >= 6.5\n",
    "    else:\n",
    "        Ttest=xxx  # aftershock time bounds for M < 6.5\n",
    "    \n",
    "    a=days[i+1:nevt]-days[i]    # time interval in days to subsequent earthquakes in catalog\n",
    "    m=mag[i+1:nevt]   # magnitudes of subsequent earthquakes in catalog\n",
    "    b=haversine_np(lon[i],lat[i],lon[i+1:nevt],lat[i+1:nevt]) # distance in km to subsequent EQs in catalog\n",
    "    \n",
    "    icnt=np.count_nonzero(a <= Ttest)   # counts the number of potential aftershocks, \n",
    "                                        # the number of intervals <= Ttest bound\n",
    "    if icnt > 0:  # if there are potential aftershocks\n",
    "        itime=np.array(np.nonzero(a <= Ttest)) + (i+1) # indices of potential aftershocks <= Ttest bound\n",
    "        for j in range(0,icnt,1):   # loops over the aftershocks         \n",
    "            if b[j] <= Dtest and m[j] < mag[i]: # test if the event is inside the distance window \n",
    "                                                # and that the event is smaller than the current main EQ\n",
    "                save[0][cnt]=itime[0][j]  # index value of the aftershock\n",
    "                cnt += 1 # increment the counting variable\n",
    "\n",
    "                \n",
    "af_ind=np.delete(np.unique(save),0)   # This is an array of indexes that will be used to delete events flagged \n",
    "                                      # as aftershocks    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `np.delete(array,indices_to_delete)` to delete the aftershock events which are located at indexes `af_ind` from your original arrays. You can also use `dataframe_name.drop(indices_to_delete,axis=0)` to delete rows from a DataFrame.\n",
    "\n",
    "<font color=goldenrod>**_Code for you to write_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the aftershock events\n",
    "declustered_df = ...\n",
    "declustered_days=...  #The aftershocks are deleted from the days array \n",
    "declustered_mag=...  #The aftershocks are deleted from the mag array \n",
    "declustered_lon=...  #The aftershocks are deleted from the lon array \n",
    "declustered_lat=...  #The aftershocks are deleted from the lat array \n",
    "n=len(declustered_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>**_Concept question:_**</font> How many of the raw events catalog were main-shocks vs. after-shocks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Magnitude vs. Time for the Declustered catalog\n",
    "\n",
    "Plot magnitude vs. time for the raw catalog and add on top the declustered catalog with slightly larger markers, so the main-shocks in the raw catalog are covered and the aftershocks remain. Add a legend to label the two sets of markers. Add labels and a title.\n",
    "\n",
    "<font color=goldenrod>**_Code for you to write_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot magnitude vs. time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>**_Concept question:_**</font> At approximately what dates (or in days since first EQ) do you observe a large aftershock sequence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Histogram of Magnitude for the Declustered Catalog\n",
    "\n",
    "Plot a histogram of the magnitudes in the raw catalog, with a log y-axis. Add a histogram of the declustered catalog on top to compare. Add a legend to label the two datasets. Add labels and a title.\n",
    "\n",
    "<font color=goldenrod>**_Code for you to write_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the B-value of the Gutenberg-Richter Law for the declustered catalog\n",
    "\n",
    "Run this code as is and inspect the B-value for the Bay Area catalog after you removed the aftershocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the observed log10 number of events per year as function of magnitude (data)\n",
    "min_mag=0.0\n",
    "max_mag=6.9\n",
    "m_declust=np.arange(min_mag,max_mag,0.1)\n",
    "N_declust=np.zeros(len(m_declust))\n",
    "numyr=2020-1910\n",
    "for i in range(0,len(m_declust),1):\n",
    "    N_declust[i]=np.log10(np.count_nonzero(declustered_mag >= m_declust[i])/numyr)\n",
    "    \n",
    "# Solve for Model Parameters\n",
    "A_declust = np.column_stack((np.ones(len(m_declust)), m_declust))\n",
    "trans_A_declust = np.transpose(A_declust)\n",
    "ATA_declust = trans_A_declust @ A_declust\n",
    "ATN_declust = trans_A_declust @ np.transpose([N_declust])\n",
    "inv_declust = np.linalg.inv(ATA_declust)\n",
    "soln_declust = inv_declust @ ATN_declust\n",
    "\n",
    "print('The declustered data B value: %7.3f'%(soln_declust[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slope (B-value) of the model line for the declustered catalog is shallower than that for the raw catalog. This indicates that there were more small aftershock events relative to larger events. Is this intuitive? \n",
    "\n",
    "<br>\n",
    "\n",
    "The A and B values can be used to predict the occurance rate of M6+ earthquakes for the Bay Area. Run this code as is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag2=m_declust[np.max(np.nonzero(m_declust < 6.1))]\n",
    "predicted_rate=10**(soln_declust[0]+soln_declust[1]*mag2)\n",
    "\n",
    "print ('Predicted M=%3.1f events per year: %4.3f, Average years between events: %4.1f'% \n",
    "       (mag2,predicted_rate,1/predicted_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last M6+ earthquake in the catalog was the 6.9 Loma Prieta Quake in 1989 (31 years ago)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map the Raw Earthquake Catalog\n",
    "\n",
    "On a map of the Bay Area plot the location of events in the declustered catalog. Scale the marker color and size by magnitude. You'll also add the locations of major faults in the area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load locations of faults to add to our map\n",
    "a=pd.read_table('data/Hayward.txt') \n",
    "hay_lon=a['x'].values\n",
    "hay_lat=a['y'].values\n",
    "\n",
    "a=pd.read_table('data/San_Andreas.txt') \n",
    "SA_lon=a['x'].values\n",
    "SA_lat=a['y'].values\n",
    "\n",
    "a=pd.read_table('data/San_Gregorio.txt') \n",
    "SG_lon=a['x'].values\n",
    "SG_lat=a['y'].values\n",
    "\n",
    "a=pd.read_table('data/Calaveras.txt') \n",
    "cal_lon=a['x'].values\n",
    "cal_lat=a['y'].values\n",
    "\n",
    "a=pd.read_table('data/Hunting_Creek.txt') \n",
    "HC_lon=a['x'].values\n",
    "HC_lat=a['y'].values\n",
    "\n",
    "a=pd.read_table('data/Rodgers_Creek.txt') \n",
    "RC_lon=a['x'].values\n",
    "RC_lat=a['y'].values\n",
    "\n",
    "a=pd.read_table('data/Concord.txt') \n",
    "con_lon=a['x'].values\n",
    "con_lat=a['y'].values\n",
    "\n",
    "a=pd.read_table('data/Greenville.txt') \n",
    "grn_lon=a['x'].values\n",
    "grn_lat=a['y'].values\n",
    "\n",
    "a=pd.read_table('data/Maacama.txt') \n",
    "m_lon=a['x'].values\n",
    "m_lat=a['y'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot map of main shock events with the marker size and color scaled and sorted by magnitude, and plot the fault locations on top. Add a legend.\n",
    "\n",
    "<font color=goldenrod>**_Code for you to write_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a Map of the earthquake catalog\n",
    "\n",
    "# Set Corners of Map\n",
    "\n",
    "\n",
    "# coordinates for UC Berkeley\n",
    "\n",
    "\n",
    "# make plot object with ticks, coastlines, etc.\n",
    "plt.figure(1,(10,10))\n",
    "\n",
    "\n",
    "\n",
    "#Sort Descending to plot largest events on top\n",
    "\n",
    "\n",
    "plt.scatter(..., ..., s=..., c=...,vmax=6.9, cmap='plasma',alpha=0.4,marker='o',label='Main Earthquakes')\n",
    "plt.plot(hay_lon,hay_lat,'-',color='red',linewidth=2,label='Hayward Fault')\n",
    "plt.plot(SA_lon,SA_lat,'-',color='orange',linewidth=2,label='San Andreas Fault')\n",
    "plt.plot(SG_lon,SG_lat,'-',color='yellow',linewidth=2,label='San Gregorio Fault')\n",
    "plt.plot(cal_lon,cal_lat,'-',color='green',linewidth=2,label='Calaveras Fault')\n",
    "plt.plot(con_lon,con_lat,'-',color='cyan',linewidth=2,label='Concord Fault')\n",
    "plt.plot(grn_lon,grn_lat,'-',color='lime',linewidth=2,label='Greenville Fault')\n",
    "plt.plot(m_lon,m_lat,'-',color='blueviolet',linewidth=2,label='Maacama Fault')\n",
    "plt.plot(RC_lon,RC_lat,'-',color='magenta',linewidth=2,label='Rodgers Creek Fault')\n",
    "plt.plot(HC_lon,HC_lat,'-',color='blue',linewidth=2,label='Hunting Creek Fault')\n",
    "plt.plot(Berk_lon,Berk_lat,'rs',markersize=8,label='UC Berkeley')  # plot red square on Berkeley Campus\n",
    "plt.colorbar(shrink=0.8,label='Magnitude')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/fault_map.png\" width=550>\n",
    "Map of Bay Area faults. \n",
    "Source: https://pubs.er.usgs.gov/publication/fs20163020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>**_Concept question:_**</font> What faults have historically been active since 1911? What mapped bay-area fault has the highest probability of rupturing with M$\\geq6.7$ in the near future?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn in this notebook\n",
    "\n",
    "Save your completed notebook. Click on __File, Download as, HTML__ to save the notebook as a HTML file. Upload it to the [bCourses assignment page](https://bcourses.berkeley.edu/courses/1498475/assignments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
